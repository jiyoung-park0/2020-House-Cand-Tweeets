{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be0cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade openai\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"[Your OpenAI API key]\"\n",
    "client = OpenAI()\n",
    "\n",
    "# Load data\n",
    "#data = pd.read_csv('Tweets_candidate_info.csv')\n",
    "\n",
    "data = pd.read_csv('New_Tweets_candidate_info.csv')       # Resume from where it stopped\n",
    "\n",
    "original_tweet_data = data['original_tweet']\n",
    "\n",
    "# Define function to remove numbers, links, %, @ from text\n",
    "def clean_text(text):\n",
    "    # Remove links\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove emails\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    # Remove % symbol\n",
    "    text = re.sub(r'%', '', text)\n",
    "    # Remove @ symbol and usernames\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Added on 08-09\n",
    "\n",
    "    # Remove dollar sign\n",
    "    text = re.sub(r'\\$', '', text)\n",
    "    \n",
    "    # Remove & symbol\n",
    "    text = re.sub(r'&', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Remove numbers, links, %, @ from original_tweet\n",
    "cleaned_tweet_data = original_tweet_data.apply(clean_text)\n",
    "\n",
    "# Check if numbers remain in the cleaned data\n",
    "remaining_numbers = cleaned_tweet_data[cleaned_tweet_data.str.contains(r'\\d', na=False)]\n",
    "print(\"Number of tweets with remaining numbers after cleaning:\", len(remaining_numbers))\n",
    "\n",
    "# Check if % symbols remain in the cleaned data\n",
    "remaining_percent = cleaned_tweet_data[cleaned_tweet_data.str.contains(r'%', na=False)]\n",
    "print(\"Number of tweets with remaining % symbols after cleaning:\", len(remaining_percent))\n",
    "\n",
    "# Check if @ symbols remain in the cleaned data\n",
    "remaining_at = cleaned_tweet_data[cleaned_tweet_data.str.contains(r'@', na=False)]\n",
    "print(\"Number of tweets with remaining @ symbols after cleaning:\", len(remaining_at))\n",
    "\n",
    "# Print examples of tweets with remaining numbers, %, @ symbols\n",
    "print(\"\\nExamples of tweets with remaining numbers:\")\n",
    "print(remaining_numbers.head())\n",
    "\n",
    "print(\"\\nExamples of tweets with remaining % symbols:\")\n",
    "print(remaining_percent.head())\n",
    "\n",
    "print(\"\\nExamples of tweets with remaining @ symbols:\")\n",
    "print(remaining_at.head())\n",
    "\n",
    "# Convert the entire data to a dataframe (skip data splitting and use the entire dataset)\n",
    "train_data = cleaned_tweet_data.to_frame(name='original_tweet')\n",
    "\n",
    "# Check for NaN values\n",
    "nan_count = train_data.isnull().sum().sum()\n",
    "print(\"Number of NaN values:\", nan_count)\n",
    "\n",
    "# Print rows with NaN values\n",
    "nan_rows = train_data[train_data.isnull().any(axis=1)]\n",
    "print(\"Rows with NaN values:\")\n",
    "print(nan_rows)\n",
    "\n",
    "# Train text classification model using OpenAI\n",
    "def classify_identity(text):\n",
    "    content = \"Search for mentions containing racial or gender identity appeals. Identity appeals include 1) highlighting positive aspects of a community's past and present or 2) discussing grievances related to a community's past or present. Code mentions as follows: 1 for racial identity appeal, 2 for gender identity appeal, and 0 for neither case.\\n\\nText: {0}\\n\\n\".format(text)\n",
    "    \n",
    "    # Get API response\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[                                                                                                                                                                                                                                                              \n",
    "            {\"role\": \"system\", \"content\": \"You are a specialist in identity recognition and text classification.\"},   # Assign persona\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Extract content from response\n",
    "    message = response.choices[0].message.content\n",
    "    \n",
    "    # Extract numbers from content string\n",
    "    category = re.sub(r'[^0-3]', '', message)\n",
    "    return category  # Return result\n",
    "\n",
    "# List to store classification results\n",
    "classified_results = []\n",
    "\n",
    "# Perform classification for each text\n",
    "save_interval = 100  # Set how often to save\n",
    "for idx, reply in enumerate(train_data['original_tweet']):\n",
    "    try:\n",
    "        category = classify_identity(reply)\n",
    "        classified_results.append(category)\n",
    "        \n",
    "        # Save progress at specified intervals\n",
    "        if (idx + 1) % save_interval == 0:\n",
    "            temp_train_data = train_data.iloc[:len(classified_results)].copy()\n",
    "            temp_train_data['category'] = classified_results\n",
    "            temp_train_data.to_csv('results_partial_temp_0810.csv', index=False)\n",
    "            print(f\"Progress saved at {idx + 1} entries.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing origin: {reply}\\n{e}\")\n",
    "        break  # Exit loop if interrupted\n",
    "\n",
    "# Add classification results to training data\n",
    "train_data = train_data.iloc[:len(classified_results)]  # Use only the results obtained so far\n",
    "train_data['category'] = classified_results\n",
    "\n",
    "# Save final results to CSV file\n",
    "train_data.to_csv('GPT_TextClassification_origin_3.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
